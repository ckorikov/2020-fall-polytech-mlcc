{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning   \n",
    "\n",
    "## What is ML?\n",
    "\n",
    ">  \"[*Machine Learning is the*] *field of study that gives computers the ability to learn without being explicitly programmed*\" *(Samuel, 1959)*  \n",
    "\n",
    "> \"*A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.*\" *(Mitchell, 1997)*  \n",
    "\n",
    "> \"*ML is a branch of AI that systematically applies algorithms to synthesize the underlying relationships among data and information*\". *(Awad & Khanna, 2015)*\n",
    "\n",
    "> \"*Machine Learning is the science (and art) of programming computers so they can learn from data.*\" *(Géron, 2017)*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Machine intelligence measure\n",
    "    Alan Turing introduced a benchmark standard for demonstrating machine intelligence: machine has to be intelligent and responsive in a manner that cannot be differentiated from that of a human being. (Turing, 1950)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "References:    \n",
    "  1. Samuel, Arthur L. “Some Studies in Machine Learning Using the Game of Checkers,” IBM Journal of R&D 44:1.2 (1959): 210–229.  \n",
    "  2. Thomas M. Mitchell. Machine Learning (1st. ed.). McGraw-Hill, Inc., USA. 1997.\n",
    "  3. Awad M., Khanna R. (2015) Machine Learning. In: Efficient Learning Machines. Apress, Berkeley, CA. https://doi.org/10.1007/978-1-4302-5990-9_1\n",
    "  4. Géron, Aurélien. Hands-on machine learning with Scikit-Learn and TensorFlow : concepts, tools, and techniques to build intelligent systems. Sebastopol, CA: O'Reilly Media, 2017. \n",
    "  5. Turing, Alan M. “Computing machinery and intelligence.” Mind (1950): 433–460.\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "## Where ML is used?\n",
    "\n",
    "* Web search engines and Ad placement  \n",
    "  * improving the search results\n",
    "  * product recommendations\n",
    "* Stock market prediction\n",
    "* Traffic prediction\n",
    "* Price forecasting\n",
    "* Weather forecasting\n",
    "* Big Data Analytics\n",
    "* Credit scoring \n",
    "* Financial cyber security  \n",
    "  * online detection and tracking monetary frauds  \n",
    "  * isolation of illegitimate transactions \n",
    "* Filtering services\n",
    "  * email spam filtering\n",
    "  * bad pop-up ads removal\n",
    "  * malware protection\n",
    "* Gene sequence analysis\n",
    "* Behavior analysis\n",
    "* Drug Development\n",
    "* Face recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Venn diagram\n",
    "\n",
    "<div style=\"width:image width px; \n",
    "            font-size:80%; \n",
    "            text-align:center; \n",
    "            float: left; padding-left-right-top-bottom:0.5em;  \n",
    "            border-style: solid; border-color: rgba(211, 211, 211, 0.000);\n",
    "            background-color: rgba(0, 0, 0, 0.000);\">\n",
    "    <img src=\"./pics/DS_VD.png\" \n",
    "         alt=\"alternate text\" \n",
    "         width=400 \n",
    "         style=\"padding-bottom:0.5em;\"/>\n",
    "    <div style=\"padding: 3px; \n",
    "                width: 400px; \n",
    "                word-wrap: break-word; \n",
    "                text-align:justify;\">\n",
    "        Illustration of DS Venn diagram by Drew Conway in 2010. <br> \n",
    "        <a href=\"http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram\" \n",
    "           style=\"float: left;\"> \n",
    "           Source \n",
    "        </a>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"width:image width px; \n",
    "            font-size:80%; \n",
    "            text-align:center; \n",
    "            float: left; padding-left-right-top-bottom:0.5em;  \n",
    "            border-style: solid; border-color: rgba(211, 211, 211, 0.000);\n",
    "            background-color: rgba(0,0, 0, 0.000;\">\n",
    "    <img src=\"./pics/DS_VD_2.jpg\" \n",
    "         alt=\"alternate text\" \n",
    "         width=400\n",
    "         style=\"padding-bottom:0.5em;\"/>\n",
    "    <div style=\"padding: 3px; \n",
    "                width: 400px; \n",
    "                word-wrap: break-word; \n",
    "                text-align:justify;\">\n",
    "        Illustration of AI/ML/DL Venn diagram by Gregory Piatetsky-Shapiro. <br> \n",
    "        <a href=\"https://www.kdnuggets.com/2016/03/data-science-puzzle-explained.html\" \n",
    "           style=\"float: left;\"> \n",
    "           Source \n",
    "        </a>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Terminology in ML\n",
    "\n",
    "* **Classifier**.  \n",
    "  A method that receives a new input as an unlabeled instance of an observation or feature and identifies a category or class to which it belongs. Many commonly used classifiers employ statistical inference (probability measure) to categorize the best label for a given instance.\n",
    "\n",
    "* **Confusion Matrix** (aka **Error Matrix**).  \n",
    "  A matrix that visualizes the performance of the classification algorithm using the data in the matrix. It compares the predicted classification against the actual classification in the form of false positive, true positive, false negative and true negative information. A confusion matrix for a two-class classifier system (Kohavi and Provost, 1998) follows: \n",
    "\n",
    "<img src=\"./pics/errormatrix.jpg\" style=\"width:400px;\">\n",
    "\n",
    "* **Accuracy** (aka **Error Rate**).  \n",
    "  The rate of correct (or incorrect) predictions made by the model over a dataset. Accuracy is usually estimated by using an independent test set that was not used at any time during the learning process. More complex accuracy estimation techniques, such as **cross-validation** and **bootstrapping**, are commonly used, especially with datasets containing a small number of **instances**.\n",
    "\n",
    "<img src=\"./pics/accuracy.jpg\" style=\"width:500px;\">\n",
    "\n",
    "where β has a value from 0 to infinity (∞) and is used to control the weight assigned to P and R.\n",
    "\n",
    "* **Cost**.  \n",
    "  The measurement of performance (or accuracy) of a model that predicts (or evaluates) the outcome for an established result; in other words, that quantifies the deviation between predicted and actual values (or class labels). An optimization function attempts to minimize the cost function.\n",
    "\n",
    "* **Cross-Validation**.  \n",
    "  A verification technique that evaluates the generalization ability of a model for an independent dataset. It defines a dataset that is used for testing the trained model during the training phase for overfitting. Cross-validation can also be used to evaluate the performance of various prediction functions. In **k-fold cross-validation**, the training dataset is arbitrarily partitioned into **k** mutually exclusive subsamples (or **folds**) of equal sizes. The model is trained **k** times (or **folds**), where each iteration uses one of the k subsamples for testing (cross-validating), and the remaining **k-1** subsamples are applied toward training the model. The k results of cross-validation are averaged to estimate the accuracy as a single estimation.\n",
    "\n",
    "* **Data Mining**.  \n",
    "  The process of knowledge discovery or pattern detection in a large dataset. The methods involved in data mining aid in extracting the accurate data and transforming it to a known structure for further evaluation.\n",
    "\n",
    "* **Dataset**.  \n",
    "  A collection of data that conform to a schema with no ordering requirements. In a typical dataset, each column represents a feature and each row represents a member of the dataset.\n",
    "\n",
    "* **Dimension**.  \n",
    "  A set of attributes that defines a property. The primary functions of dimension are filtering, classification, and grouping.\n",
    "\n",
    "* **Induction Algorithm**.  \n",
    "  An algorithm that uses the training dataset to generate a model that generalizes beyond the training dataset.\n",
    "\n",
    "* **Instance**.  \n",
    "  An object characterized by feature vectors from which the model is either trained for generalization or used for prediction.\n",
    "\n",
    "* **Knowledge discovery**.  \n",
    "  The process of abstracting knowledge from structured or unstructured sources to serve as the basis for further exploration. Such knowledge is collectively represented as a schema and can be condensed in the form of a model or models to which queries can be made for statistical prediction, evaluation, and further knowledge discovery.\n",
    "\n",
    "* **Model**.  \n",
    "  A structure that summarizes a dataset for description or prediction. Each model can be tuned to the specific requirements of an application. Applications in big data have large datasets with many predictors and **features** that are too complex for a simple parametric model to extract useful information. The learning process synthesizes the parameters and the structures of a model from a given dataset.  \n",
    "  Models may be categorized as:  \n",
    "   * **parametric** described by a finite set of parameters, such that future predictions are independent of the new dataset    \n",
    "   * **nonparametric** described by an infinite set of parameters, such that the data distribution cannot be expressed in terms of a finite set of parameters. Nonparametric models are simple and flexible, and make fewer assumptions, but they require larger datasets to derive accurate conclusions.\n",
    "\n",
    "* **Feature vector**.  \n",
    "  An n-dimensional numerical vector of explanatory variables representing an instance of some object that facilitates processing and statistical analysis. Feature vectors are often weighted to construct a predictor function that is used to evaluate the quality or fitness of the prediction. The dimensionality of a feature vector can be reduced by various dimensionality reduction techniques, such as:  \n",
    "  * **principal component analysis (PCA)**  \n",
    "  * **multilinear subspace reduction, isomaps, and latent semantic analysis (LSA)**  \n",
    "  The vector space associated with these vectors is often called the feature space.\n",
    "  \n",
    "\n",
    "References:  \n",
    "  1. Awad M., Khanna R. (2015) Machine Learning. In: Efficient Learning Machines. Apress, Berkeley, CA. https://doi.org/10.1007/978-1-4302-5990-9_1\n",
    "  2. Kohavi, Ron, and Foster Provost. “Glossary of Terms.” Machine Learning 30, no. 2–3 (1998): 271–274."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main steps in the process of developing ML algorithms:\n",
    "\n",
    "  1. **Collect the data**.  \n",
    "     Select the subset of all available data attributes that might be useful in solving the problem. Selecting all the available data may be unnecessary or counterproductive. Depending upon the problem, data can either be retrieved through a data-stream API (such as a CPU performance counters) or synthesized by combining multiple data streams. In some cases, the input data streams, whether raw or synthetic, may be statistically preprocessed to improve usage or reduce bandwidth.\n",
    "     \n",
    "     \n",
    "  2. **Preprocess the Data**.  \n",
    "     Present the data in a manner that is understood by the consumer of the data. Preprocessing consists of the following three steps:  \n",
    "     2.1. **Formatting**. The data needs to be presented in a useable format. Using an industry-standard format enable plugging the solution with multiple vendors that in turn can mix and match algorithms and data sources such as XML, HTML, and SOAP.  \n",
    "     2.2. **Cleaning**. The data needs to be cleaned by removing, substituting, or fixing corrupt or missing data. In some cases, data needs to be normalized, discretized, averaged, smoothened, or differentiated for efficient usage. In other cases, data may need to be transmitted as integers, double precisions, or strings.   \n",
    "     2.3. **Sampling**. Data need to be sampled at regular or adaptive intervals in a manner such that redundancy is minimized without the loss of information for transmission via communication channels.\n",
    "         \n",
    "     \n",
    "  3. **Transform the data**.  \n",
    "     Transform the data specific to the algorithm and the knowledge of the problem. Transformation can be in the form of feature scaling, decomposition, or aggregation. Features can be decomposed to extract the useful components embedded in the data or aggregated to combine multiple instances into a single feature.\n",
    "     \n",
    "     \n",
    "  4. **Train the algorithm**.  \n",
    "     Select the training and testing datasets from the transformed data. An algorithm is trained on the training dataset and evaluated against the test set. The transformed training dataset is fed to the algorithm for extraction of knowledge or information. This trained knowledge or information is stored as a model to be used for cross-validation and actual usage. *Unsupervised learning, having no target value, does not require the training step*.\n",
    "     \n",
    "     \n",
    "  5. **Test the algorithm**.   \n",
    "     Evaluate the algorithm to test its effectiveness and performance. This step enables quick determination whether any learnable structures can be identified in the data. A trained model exposed to test dataset is measured against predictions made on that test dataset which are indicative of the performance of the model. If the performance of the model needs improvement, repeat the previous steps by changing the data streams, sampling rates, transformations, linearizing models, outliers’ removal methodology, and biasing schemes.\n",
    "     \n",
    "     \n",
    "  6. **Execute and predict**.  \n",
    "     Apply the validated model to perform an actual task of prediction. If new data are encountered, the model is retrained by applying the previous steps. The process of training may coexist with the real task of predicting future behavior.\n",
    "     \n",
    "     \n",
    "<img src=\"./pics/01_09.png\" style=\"width:700px;\">\n",
    "\n",
    "References:    \n",
    "  1. Awad M., Khanna R. (2015) Machine Learning. In: Efficient Learning Machines. Apress, Berkeley, CA. https://doi.org/10.1007/978-1-4302-5990-9_1\n",
    "  2. Sebastian Raschka, Vahid Mirjalili. Python Machine Learning. 3rd Edition. Birmingham, UK: Packt Publishing, 2019. ISBN: 978-1789955750"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML categories\n",
    "\n",
    "* **Supervised Learning** [Обучение с учителем]  \n",
    "    * Classifictation\n",
    "    * Regression\n",
    "\n",
    "* **Unsupervised Learning** [Обучение без учителя]  \n",
    "    * Clustering\n",
    "    * Dimensionality reduction\n",
    "    \n",
    "* **Reinforcement Learning (RL)** [Обучение с подкреплением]  \n",
    "\n",
    "\n",
    "<img src=\"./pics/ml_types_usage.png\" style=\"width:700px;\">\n",
    "\n",
    "\n",
    "## Fundamental ML Algorithms\n",
    "\n",
    "* Linear Regression\n",
    "* Logistic Regression\n",
    "* k-Means Clustering\n",
    "* k-Nearest Neighbors (kNN)\n",
    "* Support Vector Machines (SVM)\n",
    "* Decision Trees\n",
    "* Random Forests (RF)\n",
    "* Naive Bayes \n",
    "* Ensembles\n",
    "* Boosting\n",
    "* Dimensionality Reduction\n",
    "* Neural Networks (NN) \n",
    "* Deep Learning (DL)\n",
    "* Reinforcement Learning (RL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning\n",
    "<img src=\"./pics/sup-unsup.jpeg\" style=\"width:400px;\">\n",
    "<img src=\"./pics/class-regress.png\" style=\"width:600px;\">\n",
    "<img src=\"./pics/class-regress_example1.png\" style=\"width:400px;\">\n",
    "\n",
    "* Regression  \n",
    "    These algorithms are normally useful for predicting a single number. If you needed to create an algorithm that predicted a stock price based on features of stocks, you would select this type of model. These are called *continuous variables*.\n",
    "    \n",
    "   * Linear\n",
    "       Model: y=ax+b => y=β₀+β₁x₁+…+βᵢxᵢ,   \n",
    "       where β₀ is the y-intercept, the y-value when all explanatory variables are set to zero. β₁ to βᵢ are the coefficients for variables x₁ to xᵢ, the amount y increases or decreases with a one unit change in that variable, assuming that all other variables are held constant. For example, if the equation was y=1+2x₁+3x₂ then y would increase from 1 to 3 if x₁ increased from 0 to 1 and x₂ stayed at 0.\n",
    "   * Logistic  \n",
    "       Model: y= 1 / (1+e^-(β₀+β₁x₁+…+βᵢxᵢ)),  \n",
    "       which constrains it to values between 0 and 1.  \n",
    "       For this reason, it’s mostly used for binary target variables where the possible values are zero or one or where the target is the probability of a binary variable. As mentioned earlier, the equation keeps predictions from being illogical in the sense of having probabilities below 0 or higher than 1.\n",
    "       \n",
    "       \n",
    "<img src=\"./pics/regr_lin_log.jpeg\" style=\"width:400px;\">\n",
    "   \n",
    "   \n",
    "* Classification  \n",
    "   These algorithms are used to predict a member of a class of possible answers. This could be a simple \"yes or no\" classification (*binary* classification, when you have 2 classes), or \"red, green or blue.\" (*multi-class* classification, when you have multiple classes). If you needed to predict whether an unknown person was male or female from features, you would select this type of model. These are called *discrete variables*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning\n",
    "\n",
    "* Unsupervised machine learning tries to score more points for artificial intelligence without any human touch. Unsupervised machine learning algorithms rely on data that has no labels, predefined features, or specified classification sets.\n",
    "\n",
    "<img src=\"./pics/unsup_example1.png\" style=\"width:600px;\">\n",
    "\n",
    "\n",
    "<img src=\"./pics/unsup_scheme.jpg\" style=\"width:800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised vs Unsupervised\n",
    "\n",
    "\n",
    "* k-means example\n",
    "\n",
    "<img src=\"./pics/sup_unsup_example1.jpg\" style=\"width:800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Book recommendations\n",
    "* **[en]** [Python Machine Learning - 3rd edition](https://sebastianraschka.com/books.html) by Sebastian Raschka , Vahid Mirjalili (2020)\n",
    "* **[en]** [Hands-On Machine Learning with Scikit-Learn and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) by Aurélien Géron (2019)\n",
    "\n",
    "### Online cources recommendations   \n",
    "* **[ru]** [Advanced Machine Learning Specialization](https://www.coursera.org/specializations/aml?ranMID=40328) **(HSE)**  \n",
    "* **[en]** [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning) **(deeplearning.ai)** by [Andrew Ng](https://scholar.google.com/citations?user=mG4imMEAAAAJ&hl=en)\n",
    "* **[en]** [Natural Language Processing Specialization](https://www.coursera.org/specializations/natural-language-processing) **(deeplearning.ai)**\n",
    "* **[en]** [MicroMasters® in Statistics and Data Science](https://www.edx.org/micromasters/mitx-statistics-and-data-science) **(MIT)** at **EdX** \n",
    "\n",
    "\n",
    "### Sources for datasets\n",
    "* [Kaggle](https://www.kaggle.com/datasets)  (sometimes with codes/notebooks)\n",
    "* [data.world](https://data.world/)\n",
    "* [ U.S. Government’s open data](https://catalog.data.gov/dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like?  \n",
    "-  [x] Yes  \n",
    "-  [ ] No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```css\n",
    "def ml(self):\n",
    "    mlcc = self.mlcc\n",
    "return mlcc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
